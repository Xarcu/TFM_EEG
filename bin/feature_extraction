# import os
# import argparse
# import mne
# import numpy as np
# import pandas as pd
# import json
# import matplotlib.pyplot as plt
# from scipy.stats import kurtosis, skew
# from antropy import entropy as ent
# from antropy import higuchi_fd

# def plot_eeg(raw, subject_folder, plot_dir):
#     """ Plot raw EEG data and save the plot to the specified directory. """
#     plt.figure(figsize=(10, 6))
#     raw.plot(scalings='auto', show=False, block=False)
#     plot_path = os.path.join(plot_dir, f"{subject_folder}_eeg_plot.png")
#     plt.savefig(plot_path)
#     plt.close()
#     print(f"[DEBUG] EEG plot saved to {plot_path}")

# def fractal_dimension_higuchi(signal, kmax=10):
#     """ Calculate the Higuchi fractal dimension of a signal. """
#     return higuchi_fd(signal, kmax)

# def hurst_exponent(signal):
#     """ Calculate the Hurst exponent of a signal. """
#     N = len(signal)
#     T = np.arange(1, N + 1)
#     Y = np.cumsum(signal - np.mean(signal))
#     R = np.max(Y) - np.min(Y)
#     S = np.std(signal)
#     return np.log(R / S) / np.log(N) if S != 0 else np.nan

# def fuzzy_entropy(signal, m=2, r=0.2):
#     """ Compute fuzzy entropy with a specified embedding dimension m and tolerance r. """
#     N = len(signal)
#     r *= np.std(signal)
#     def _phi(m):
#         X = np.array([signal[i: N - m + i + 1] for i in range(m)]).T
#         C = np.sum(np.exp(-(np.abs(X[:, None] - X[None, :]) ** 2).sum(axis=2) / (2 * r ** 2)), axis=0)
#         return C / (N - m + 1)
#     return -np.log(_phi(m + 1).mean() / _phi(m).mean())

# def extract_features_from_events(raw, events, window_duration=1.5, sfreq=250):
#     """ Extract features from each event and store them with separate rows for each channel. """
#     features_list = []
#     max_time = raw.n_times / sfreq
#     print(f"[DEBUG] Data duration: {max_time}s, Sampling rate: {sfreq}Hz")

#     event_times_sec = events - events.min()
#     print(f"Adjusted Event Times (in seconds): {event_times_sec}")

#     out_of_bounds_events = event_times_sec[(event_times_sec < 0) | (event_times_sec > max_time)]
#     if len(out_of_bounds_events) > 0:
#         print("Out of bounds events found:", out_of_bounds_events)
#     else:
#         print("All events are within the EEG data duration.")

#     for idx, timestamp in enumerate(event_times_sec):
#         if timestamp < 0 or timestamp + window_duration > max_time:
#             continue  # Skip out-of-bounds events

#         start_sample = int(timestamp * sfreq)
#         end_sample = start_sample + int(window_duration * sfreq)
#         data_window, _ = raw[:, start_sample:end_sample]

#         for ch_idx, ch_data in enumerate(data_window):
#             features = {
#                 'channel': raw.ch_names[ch_idx],
#                 'rms': np.sqrt(np.mean(ch_data ** 2)),
#                 'ptp': np.ptp(ch_data),
#                 'skewness': skew(ch_data),
#                 'kurtosis': kurtosis(ch_data),
#                 'hurst_exponent': hurst_exponent(ch_data),
#                 'fractal_dimension': fractal_dimension_higuchi(ch_data)
#             }

#             psd, freqs = mne.time_frequency.psd_array_multitaper(
#                 ch_data[None, :], sfreq, fmin=0.5, fmax=40, verbose=False
#             )
#             features.update({
#                 'delta_power': psd[0, (freqs >= 0.5) & (freqs < 4)].mean(),
#                 'theta_power': psd[0, (freqs >= 4) & (freqs < 8)].mean(),
#                 'alpha_power': psd[0, (freqs >= 8) & (freqs < 13)].mean(),
#                 'beta_power': psd[0, (freqs >= 13) & (freqs < 30)].mean(),
#                 'gamma_power': psd[0, (freqs >= 30) & (freqs < 40)].mean()
#             })

#             try:
#                 features.update({
#                     'sample_entropy': ent.sample_entropy(ch_data, 2, 0.2 * np.std(ch_data), metric='euclidean'),
#                     'fuzzy_entropy': fuzzy_entropy(ch_data, 2, 0.2),
#                     'tsallis_entropy': ent.tsallis_entropy(ch_data, 2),
#                     'improved_multiscale_permutation_entropy': ent.improved_multiscale_permutation_entropy(ch_data, metric='euclidean'),
#                     'multiscale_fuzzy_entropy': ent.multiscale_fuzzy_entropy(ch_data, 2, 0.2 * np.std(ch_data), metric='euclidean'),
#                     'refined_multiscale_fuzzy_entropy': ent.refined_multiscale_fuzzy_entropy(ch_data, 2, 0.2 * np.std(ch_data), metric='euclidean')
#                 })
#             except Exception as e:
#                 print(f"[WARNING] Entropy calculation error: {e}")

#             features_list.append(features)
    
#     return features_list

# def load_and_extract_features(base_dir, plot_dir, sfreq=250):
#     """ Load .fif files and extract features, adding 'side' and 'channel' for each trial. """
#     left_dir = os.path.join(base_dir, 'L')
#     right_dir = os.path.join(base_dir, 'R')
#     all_features = []

#     for side, side_dir in zip(["left", "right"], [left_dir, right_dir]):
#         if not os.path.exists(side_dir):
#             print(f"[ERROR] Directory not found: {side_dir}")
#             continue
        
#         for subject_folder in sorted(os.listdir(side_dir)):
#             subject_path = os.path.join(side_dir, subject_folder)
#             if not os.path.isdir(subject_path):
#                 continue  # Skip non-directory items
            
#             raw_file, npz_file = None, None
#             for root, _, files in os.walk(subject_path):
#                 for file in files:
#                     if file.endswith('_raw.fif'):
#                         raw_file = os.path.join(root, file)
#                     elif file.endswith('_event_data.npz'):
#                         npz_file = os.path.join(root, file)
            
#             if raw_file and npz_file:
#                 try:
#                     raw = mne.io.read_raw_fif(raw_file, preload=True, verbose=False)
#                     plot_eeg(raw, subject_folder, plot_dir)  # Plot EEG data

#                     npz_data = np.load(npz_file, allow_pickle=True)
#                     events_data = np.array(npz_data['event_timestamps'], dtype=float)
                    
#                     features = extract_features_from_events(raw, events_data, window_duration=0.5, sfreq=sfreq)
#                     for feat in features:
#                         feat['side'] = side  # Add side info
#                         feat['subject'] = subject_folder  # Track subject
#                         all_features.append(feat)
#                 except Exception as e:
#                     print(f"[ERROR] Error processing subject {subject_folder}: {e}")
#             else:
#                 print(f"[DEBUG] Missing files for subject {subject_folder}.")
    
#     return all_features

# def save_features(features, output_path, file_format="parquet"):
#     """Save the extracted features to a specified format with a side column for left/right info."""
#     print(f"[DEBUG] Saving features to {output_path} as {file_format} format.")
#     features_df = pd.DataFrame(features)
    
#     if file_format == "parquet":
#         if not output_path.endswith(".parquet"):
#             output_path += ".parquet"
#         features_df.to_parquet(output_path)
#     elif file_format == "csv":
#         if not output_path.endswith(".csv"):
#             output_path += ".csv"
#         features_df.to_csv(output_path, index=False)
#     elif file_format == "json":
#         if not output_path.endswith(".json"):
#             output_path += ".json"
#         features_df.to_json(output_path, orient='records')
#     print(f"[DEBUG] Features saved successfully in {file_format} format at {output_path}")

# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description="Extract features from .fif files based on event timestamps.")
#     parser.add_argument('-f', '--base_dir', required=True, help='Base directory containing LEFT and RIGHT processed files.')
#     parser.add_argument('-o', '--output_path', required=True, help='Output path for saving the extracted features.')
#     parser.add_argument('--file_format', choices=['json', 'csv', 'parquet'], default='parquet', help="Output format for the features.")
#     parser.add_argument('-p', '--plot_dir', required=True, help='Directory to save plots of .fif files.')
#     args = parser.parse_args()

#     # Extract features and store them in a list of dictionaries
#     features = load_and_extract_features(args.base_dir, args.plot_dir, sfreq=250)

#     # Save the features
#     save_features(features, args.output_path, file_format=args.file_format)


import os
import argparse
import mne
import numpy as np
import pandas as pd
import json
import matplotlib.pyplot as plt
from scipy.stats import kurtosis, skew, entropy
from pyeeg import samp_entropy, permutation_entropy
from antropy import higuchi_fd

def plot_eeg(raw, subject_folder, plot_dir):
    """ Plot raw EEG data and save the plot to the specified directory. """
    plt.figure(figsize=(10, 6))
    raw.plot(scalings='auto', show=False, block=False)
    plot_path = os.path.join(plot_dir, f"{subject_folder}_eeg_plot.png")
    plt.savefig(plot_path)
    plt.close()
    print(f"[DEBUG] EEG plot saved to {plot_path}")

def fractal_dimension_higuchi(signal, kmax=10):
    """ Calculate the Higuchi fractal dimension of a signal. """
    return higuchi_fd(signal, kmax)

def hurst_exponent(signal):
    """ Calculate the Hurst exponent of a signal. """
    N = len(signal)
    T = np.arange(1, N + 1)
    Y = np.cumsum(signal - np.mean(signal))
    R = np.max(Y) - np.min(Y)
    S = np.std(signal)
    return np.log(R / S) / np.log(N) if S != 0 else np.nan

def lempel_ziv_complexity(signal):
    """ Calculate Lempel-Ziv complexity of a signal. """
    signal_str = ''.join(str(int(x)) for x in (signal > np.mean(signal)))
    i, lz_complexity, n = 0, 1, len(signal_str)
    while i < n - 1:
        for j in range(1, n - i):
            if signal_str[i:i + j] not in signal_str[:i]:
                lz_complexity += 1
                break
        i += 1
    return lz_complexity / len(signal_str) if len(signal_str) > 0 else 0

def extract_features_from_events(raw, events, window_duration=1.5, sfreq=250):
    """ Extract features from each event and store them with separate rows for each channel. """
    features_list = []
    max_time = raw.n_times / sfreq
    print(f"[DEBUG] Data duration: {max_time}s, Sampling rate: {sfreq}Hz")

    event_times_sec = events - events.min()
    print(f"Adjusted Event Times (in seconds): {event_times_sec}")

    out_of_bounds_events = event_times_sec[(event_times_sec < 0) | (event_times_sec > max_time)]
    if len(out_of_bounds_events) > 0:
        print("Out of bounds events found:", out_of_bounds_events)
    else:
        print("All events are within the EEG data duration.")

    for idx, timestamp in enumerate(event_times_sec):
        if timestamp < 0 or timestamp + window_duration > max_time:
            continue  # Skip out-of-bounds events

        start_sample = int(timestamp * sfreq)
        end_sample = start_sample + int(window_duration * sfreq)
        data_window, _ = raw[:, start_sample:end_sample]

        for ch_idx, ch_data in enumerate(data_window):
            features = {
                'channel': raw.ch_names[ch_idx],
                'rms': np.sqrt(np.mean(ch_data ** 2)),
                'ptp': np.ptp(ch_data),
                'skewness': skew(ch_data),
                'kurtosis': kurtosis(ch_data),
                'hurst_exponent': hurst_exponent(ch_data),
            }

            try:
                # Higuchi fractal dimension
                features['fractal_dimension'] = fractal_dimension_higuchi(ch_data)
            except Exception as e:
                print(f"[WARNING] Higuchi fractal dimension error: {e}")
                features['fractal_dimension'] = np.nan

            psd, freqs = mne.time_frequency.psd_array_multitaper(
                ch_data[None, :], sfreq, fmin=0.5, fmax=40, verbose=False
            )
            features.update({
                'delta_power': psd[0, (freqs >= 0.5) & (freqs < 4)].mean(),
                'theta_power': psd[0, (freqs >= 4) & (freqs < 8)].mean(),
                'alpha_power': psd[0, (freqs >= 8) & (freqs < 13)].mean(),
                'beta_power': psd[0, (freqs >= 13) & (freqs < 30)].mean(),
                'gamma_power': psd[0, (freqs >= 30) & (freqs < 40)].mean()
            })

            try:
                features.update({
                    'sample_entropy': samp_entropy(ch_data, 2, 0.2 * np.std(ch_data)),
                    'permutation_entropy': permutation_entropy(ch_data, 3, 1),
                })

                # Shannon entropy using SciPy
                hist, bin_edges = np.histogram(ch_data, bins=50, density=True)
                features['shannon_entropy'] = entropy(hist)

                # Lempel-Ziv Complexity
                features['lziv_complexity'] = lempel_ziv_complexity(ch_data)
            except Exception as e:
                print(f"[WARNING] Entropy calculation error: {e}")

            features_list.append(features)

    return features_list


def load_and_extract_features(base_dir, plot_dir, sfreq=250):
    """ Load .fif files and extract features, adding 'side' and 'channel' for each trial. """
    left_dir = os.path.join(base_dir, 'L')
    right_dir = os.path.join(base_dir, 'R')
    all_features = []

    for side, side_dir in zip(["left", "right"], [left_dir, right_dir]):
        if not os.path.exists(side_dir):
            print(f"[ERROR] Directory not found: {side_dir}")
            continue
        
        for subject_folder in sorted(os.listdir(side_dir)):
            subject_path = os.path.join(side_dir, subject_folder)
            if not os.path.isdir(subject_path):
                continue  # Skip non-directory items
            
            raw_file, npz_file = None, None
            for root, _, files in os.walk(subject_path):
                for file in files:
                    if file.endswith('_raw.fif'):
                        raw_file = os.path.join(root, file)
                    elif file.endswith('_event_data.npz'):
                        npz_file = os.path.join(root, file)
            
            if raw_file and npz_file:
                try:
                    raw = mne.io.read_raw_fif(raw_file, preload=True, verbose=False)
                    plot_eeg(raw, subject_folder, plot_dir)  # Plot EEG data

                    npz_data = np.load(npz_file, allow_pickle=True)
                    events_data = np.array(npz_data['event_timestamps'], dtype=float)
                    
                    features = extract_features_from_events(raw, events_data, window_duration=0.5, sfreq=sfreq)
                    for feat in features:
                        feat['side'] = side  # Add side info
                        feat['subject'] = subject_folder  # Track subject
                        all_features.append(feat)
                except Exception as e:
                    print(f"[ERROR] Error processing subject {subject_folder}: {e}")
            else:
                print(f"[DEBUG] Missing files for subject {subject_folder}.")
    
    return all_features

def save_features(features, output_path, file_format="parquet"):
    """Save the extracted features to a specified format with a side column for left/right info."""
    print(f"[DEBUG] Saving features to {output_path} as {file_format} format.")
    features_df = pd.DataFrame(features)
    
    if file_format == "parquet":
        if not output_path.endswith(".parquet"):
            output_path += ".parquet"
        features_df.to_parquet(output_path)
    elif file_format == "csv":
        if not output_path.endswith(".csv"):
            output_path += ".csv"
        features_df.to_csv(output_path, index=False)
    elif file_format == "json":
        if not output_path.endswith(".json"):
            output_path += ".json"
        features_df.to_json(output_path, orient='records')
    print(f"[DEBUG] Features saved successfully in {file_format} format at {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extract features from .fif files based on event timestamps.")
    parser.add_argument('-f', '--base_dir', required=True, help='Base directory containing LEFT and RIGHT processed files.')
    parser.add_argument('-o', '--output_path', required=True, help='Output path for saving the extracted features.')
    parser.add_argument('--file_format', choices=['json', 'csv', 'parquet'], default='parquet', help="Output format for the features.")
    parser.add_argument('-p', '--plot_dir', required=True, help='Directory to save plots of .fif files.')
    args = parser.parse_args()

    # Extract features and store them in a list of dictionaries
    features = load_and_extract_features(args.base_dir, args.plot_dir, sfreq=250)

    # Save the features
    save_features(features, args.output_path, file_format=args.file_format)
